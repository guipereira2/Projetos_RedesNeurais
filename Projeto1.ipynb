{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Projeto 1 introdução a redes neurais"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objetivo: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementar uma rede MLP, em python, sem usar pacotes prontos (e.g., Pytorch, Tensorflow, etc.)\n",
    "Com a rede implementada, desenvolver dois modelos: um para classificação e um para regressão.\n",
    "Avaliar os hiperparâmetros dos modelos variando o número de camadas, número de neurônio e taxas (eta e momentum)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importanto as bibliotecas necessárias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Funções de ativação "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A função linear retorna o próprio x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear(x): \n",
    "    return x "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A função degrau acima retorna 1 se x > 0 ou 0 caso contrário"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def degrau(x): \n",
    "    return np.where(x >= 0, 1, 0) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A função sigmoid logística retorna \n",
    "$$\n",
    "f(x) = \\frac{1}{1 + \\exp(-x)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x): \n",
    "    return 1 / (1 + np.exp(-x)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A função tangente hiperbólica retorna a tangente hiperbólica do x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tanh(x):\n",
    "    return np.tanh(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A função relu retorna o máximo entre o X e 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(x): \n",
    "    return np.maximum(0, x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A função softmax converte a saída padrão em uma distribuição de probabilidade, da forma: \n",
    "$$\n",
    "f(y) = \\frac{\\exp(y)}{\\sum_{k} \\exp(y_k)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x): \n",
    "    exp = np.exp(x - np.max(x))\n",
    "    return exp / np.sum(exp, axis=-1, keepdims=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retropropagação"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A retropropagação surgiu para resolver o problema de como atualizar os neurônios das camadas ocultas em redes neurais. Em redes profundas, esses neurônios não têm erros diretamente observáveis como na camada de saída. O objetivo da retropropagação é calcular esses erros e ajustar os pesos das camadas ocultas utilizando o método de gradiente descendente, permitindo que redes com múltiplas camadas aprendam de forma eficiente. As contas da retropropagação envolvem o cálculo do gradiente da função de erro em relação aos pesos, ajustando-os de forma iterativa para minimizar a diferença entre as saídas desejadas e as reais. Esses cálculos incluem a atualização dos pesos na camada de saída, com base no erro direto, e nas camadas ocultas, com base nos erros propagados pela rede, até a camada de entrada. O processo é feito usando o gradiente local, o qual depende da função de ativação de cada neurônio, garantindo que o erro seja distribuído corretamente entre as camadas da rede."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A fórmula para a atualização dos pesos de um neurônio, é dada por:\n",
    "$$\n",
    "\\Delta w_{kj} = \\eta \\cdot \\delta_k \\cdot x_j\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Onde: \n",
    "- $\\eta$ é a taxa de aprendizado.\n",
    "- $\\delta_k$ é o gradiente local do neurônio $k$.\n",
    "- $x_j$ é a saída do neurônio anterior $j$ (entrada para o neurônio $k$)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para k sendo um neurônio de saída: \n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A fórmula de erro quadrático médio ou MSE, é dada por: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "E(n) = \\frac{1}{2} \\sum_{k} \\left( d_k(n) - y_k(n) \\right)^2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Onde: \n",
    "- $E(n)$ é a função de erro na época $n$.\n",
    "- $d_k(n)$ é o valor desejado para o neurônio $k$ na época $n$.\n",
    "- $y_k(n)$ é o valor atual da saída do neurônio $k$ na época $n$.\n",
    "- A soma é realizada sobre todos os neurônios $k$ da camada de saída."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def retropropagação(X, y, pesos, bias, aprendizado): \n",
    "    #Feedforward: as entradas se propagam pela rede, camada a camada, da entrada até a saída"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
