{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Projeto 1 introdução a redes neurais**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objetivo: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementar uma rede MLP, em python, sem usar pacotes prontos (e.g., Pytorch, Tensorflow, etc.)\n",
    "Com a rede implementada, desenvolver dois modelos: um para classificação e um para regressão.\n",
    "Avaliar os hiperparâmetros dos modelos variando o número de camadas, número de neurônio e taxas (eta e momentum)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importanto as bibliotecas necessárias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Funções de ativação "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A função linear retorna o próprio x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear(x): \n",
    "    return x "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A função degrau acima retorna 1 se x > 0 ou 0 caso contrário"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def degrau(x): \n",
    "    return np.where(x >= 0, 1, 0) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A função sigmoid logística retorna \n",
    "$$\n",
    "f(x) = \\frac{1}{1 + \\exp(-x)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x): \n",
    "    return 1 / (1 + np.exp(-x)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A derivada da função sigmoid retorna \n",
    "\n",
    "$$\n",
    "f'(x) = f(x) \\cdot (1 - f(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid_derivada(x): \n",
    "    return x * (1 - x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A função tangente hiperbólica retorna a tangente hiperbólica do x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tanh(x):\n",
    "    return np.tanh(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A função relu retorna o máximo entre o X e 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(x): \n",
    "    return np.maximum(0, x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A função softmax converte a saída padrão em uma distribuição de probabilidade, da forma: \n",
    "$$\n",
    "f(y) = \\frac{\\exp(y)}{\\sum_{k} \\exp(y_k)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x): \n",
    "    exp = np.exp(x - np.max(x))\n",
    "    return exp / np.sum(exp, axis=-1, keepdims=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retropropagação"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A retropropagação surgiu para resolver o problema de como atualizar os neurônios das camadas ocultas em redes neurais. Em redes profundas, esses neurônios não têm erros diretamente observáveis como na camada de saída. O objetivo da retropropagação é calcular esses erros e ajustar os pesos das camadas ocultas utilizando o método de gradiente descendente, permitindo que redes com múltiplas camadas aprendam de forma eficiente. As contas da retropropagação envolvem o cálculo do gradiente da função de erro em relação aos pesos, ajustando-os de forma iterativa para minimizar a diferença entre as saídas desejadas e as reais. Esses cálculos incluem a atualização dos pesos na camada de saída, com base no erro direto, e nas camadas ocultas, com base nos erros propagados pela rede, até a camada de entrada. O processo é feito usando o gradiente local, o qual depende da função de ativação de cada neurônio, garantindo que o erro seja distribuído corretamente entre as camadas da rede."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A fórmula para a atualização dos pesos de um neurônio, é dada por:\n",
    "$$\n",
    "\\Delta w_{kj} = \\eta \\cdot \\delta_k \\cdot x_j\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Onde: \n",
    "    - $\\eta$ é a taxa de aprendizado\n",
    "    - $\\delta_k$ é o gradiente local do neurônio $k$\n",
    "    - $x_j$ é a saída do neurônio anterior $j$ (entrada para o neurônio $k$)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### O cálculo do gradiente local ($\\delta_k$) é dado por dois casos: \n",
    "\n",
    "#### Neurônio de saída: \n",
    "$\n",
    "\\delta_k = e_k \\cdot f'(v_k)\n",
    "$\n",
    "- Onde:\n",
    "  - $\\delta_k$: Gradiente local do neurônio $k$\n",
    "  - $e_k$: Erro no neurônio de saída\n",
    "  - $f'(v_k)$: Derivada da função de ativação com relação ao estado interno $v_k$\n",
    "\n",
    "\n",
    "#### Caso 2: Neurônio oculto: \n",
    "$\n",
    "\\delta_j = f'(v_j) \\cdot \\sum_k \\delta_k \\cdot w_{kj}\n",
    "$\n",
    "\n",
    "- Onde:\n",
    "  - $\\delta_j$: Gradiente local do neurônio $j$ da camada oculta\n",
    "  - $f'(v_j)$: Derivada da função de ativação com relação ao estado interno $v_j$\n",
    "  - $\\sum_k$: Soma sobre todos os neurônios $k$ da camada seguinte\n",
    "  - $\\delta_k$: Gradiente local do neurônio $k$ da camada seguinte\n",
    "  - $w_{kj}$: Peso entre o neurônio $j$ da camada oculta e o neurônio $k$ da camada seguinte\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A fórmula de erro quadrático médio ou MSE, é dada por: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "E(n) = \\frac{1}{2} \\sum_{k} \\left( d_k(n) - y_k(n) \\right)^2\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Onde: \n",
    "- $E(n)$ é a função de erro na época $n$.\n",
    "- $d_k(n)$ é o valor desejado para o neurônio $k$ na época $n$.\n",
    "- $y_k(n)$ é o valor atual da saída do neurônio $k$ na época $n$.\n",
    "- A soma é realizada sobre todos os neurônios $k$ da camada de saída."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feedfoward e Feedback "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "O algoritmo de retropropagação opera em duas fases principais. Na fase de feedforward, os dados de entrada percorrem a rede camada por camada até a saída, produzindo as previsões. Já na fase de feedback, os erros calculados na saída são propagados de volta, ajustando os pesos camada a camada. Esse processo possibilita calcular o gradiente dos neurônios ocultos e atualizar seus pesos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pseudocódigo "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Inicialização**: Configure os hiperparâmetros (taxa de aprendizado, número máximo de épocas, etc.) e inicialize os pesos da rede.\n",
    "- **Treinamento**: Repita os passos abaixo até que o erro seja menor ou igual à tolerância definida ou até atingir o número máximo de épocas:\n",
    "   - Aplique um padrão de entrada $x_i$ e o respectivo vetor de saída desejado $d_i$.\n",
    "   - Realize a **propagação do sinal** (*feedforward*) da entrada até a saída.\n",
    "   - Execute a **retropropagação dos erros** da saída para as camadas anteriores.\n",
    "   - Atualize os pesos da rede com base nos gradientes calculados.\n",
    "   - Retorne ao passo inicial para o próximo padrão de entrada."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementação "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inicializando o momentum acumulado  \n",
    "momentum_acumulado = None \n",
    "\n",
    "def retropropagação(X, y, pesos, bias, taxa_de_aprendizado, momentum): \n",
    "    \"\"\"\n",
    "    Função de retropropagação com feedforward e backpropagation\n",
    "    \n",
    "    Parâmetros:\n",
    "    X: matriz de entradas (tamanho: número de amostras x número de características)\n",
    "    y: vetor de saídas (tamanho: número de amostras)\n",
    "    pesos: pesos da rede (incluindo pesos das camadas)\n",
    "    bias: valores de bias das camadas\n",
    "    taxa_aprendizado: taxa de aprendizado (eta)\n",
    "    momentum: fator de momentum\n",
    "    \n",
    "    Retorna:\n",
    "    pesos atualizados e bias ajustado\n",
    "    \"\"\"\n",
    "    global momentum_acumulado\n",
    "\n",
    "    # Inicializando o momentum acumulado se necessário (apenas na primeira execução)\n",
    "\n",
    "    if momentum_acumulado is None: \n",
    "        momentum_acumulado = [np.zeros_like(peso) for peso in pesos] # Inicializa o momentum com o mesmo formato dos pesos\n",
    "\n",
    "    #----------------------------------------------------------Feedforward---------------------------------------------------------------------- \n",
    "    camada_de_entrada = X # Dados de entrada X\n",
    "    ativações = [X] # Lista que armazena as ativações, começando com a entrada\n",
    "    lista_peso = [] # lista que armazena o peso de cada camada\n",
    "\n",
    "\n",
    "    for peso, bias in zip(pesos, bias): \n",
    "        w = np.dot(camada_de_entrada, peso) + bias # Calcula o w da camada\n",
    "        lista_peso.append(w) # Armazena na lista de pesos\n",
    "        camada_de_entrada = sigmoid(w) #Aplicando a função de ativação sigmoid  \n",
    "        ativações.append(camada_de_entrada) # Armazenando a ativação da camada \n",
    "\n",
    "    #----------------------------------------------------------Backpropagation------------------------------------------------------------------\n",
    "    # Calculo do erro na camada de saída \n",
    "    gradiente = (ativações[-1] - y) * sigmoid_derivada(ativações[-1]) # Erro na camada de saída\n",
    "    gradiente_w = [np.dot(ativações[-2].T, gradiente)] # Gradiente dos pesos da camada de saída\n",
    "    gradiente_bias = [np.sum(gradiente, axis=0, keepdims=True)] # Gradiente dos bias da camada de saída\n",
    "\n",
    "    # Calculando os gradientes para as camadas ocultas\n",
    "    for camada in range(2, len(pesos) + 1):  \n",
    "        w = lista_peso[-camada] # Obtém os pesos da camada atual na ordem inversa (da saída para a entrada)\n",
    "        sig = sigmoid_derivada(sigmoid(w)) # Derivada da funcao de ativação sigmoid\n",
    "        gradiente = np.dot(gradiente, pesos[-camada + 1].T) * sig # Propagando o erro\n",
    "        gradiente_w.insert(0, np.dot(ativações[-camada - 1].T, gradiente)) # Gradiente dos pesos \n",
    "        gradiente_bias.insert(0, np.sum(gradiente, axis=0, keepdims=True)) # Gradiente dos vieses\n",
    "\n",
    "    # Atualizando pesos e bias com momentum\n",
    "    for i in range(len(pesos)): \n",
    "        # Atualiza o momentum acumulado\n",
    "        momentum_acumulado[i] = momentum * momentum_acumulado[i] + gradiente_w[i] # Acumula o momentum\n",
    "        pesos[i] -= taxa_de_aprendizado * momentum_acumulado[i] # Atualiza os pesos com o momentum acumulado\n",
    "        bias[i] -= taxa_de_aprendizado * gradiente_bias[i] # Atualiza o bias\n",
    "\n",
    "    return pesos, bias # Retorna os pesos e o bias ajustado"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inicialização dos pesos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inicializar_pesos(tamanho_camadas): \n",
    "    \"\"\"\n",
    "    Inicializa os pesos e os bias de uma rede neural   \n",
    "    \n",
    "    Parâmetros: \n",
    "    tamanho_camadas: contém o número de neurônios em cada camada da rede\n",
    "    \n",
    "    Retorna: \n",
    "    pesos: lista de matrizes de pesos entre cada camada\n",
    "    bias: lista de vetores de bias para cada camada\n",
    "    \"\"\"\n",
    "    # Listas para armazenar os pesos e bias de cada camada\n",
    "    pesos = []\n",
    "    bias = []\n",
    "\n",
    "    # Itera sobre cada par de camadas consecutivas (entrada e saída)\n",
    "    for i in range(1, len(tamanho_camadas)):\n",
    "        camada_entrada = tamanho_camadas[i - 1]\n",
    "        camada_saida = tamanho_camadas[i]\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
